{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename all _nltk with _spacy to get the results for Spacy based preprocessed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>headline_cleaned</th>\n",
       "      <th>tokenized_text_nltk</th>\n",
       "      <th>lemmatized_text_nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>former versace store clerk sues over secret bl...</td>\n",
       "      <td>['former', 'versace', 'store', 'clerk', 'sues'...</td>\n",
       "      <td>['former', 'versace', 'store', 'clerk', 'sue',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>the roseanne revival catches up to our thorny ...</td>\n",
       "      <td>['the', 'roseanne', 'revival', 'catches', 'up'...</td>\n",
       "      <td>['roseanne', 'revival', 'catch', 'thorny', 'po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>mom starting to fear sons web series closest t...</td>\n",
       "      <td>['mom', 'starting', 'to', 'fear', 'sons', 'web...</td>\n",
       "      <td>['mom', 'starting', 'fear', 'son', 'web', 'ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>boehner just wants wife to listen not come up ...</td>\n",
       "      <td>['boehner', 'just', 'wants', 'wife', 'to', 'li...</td>\n",
       "      <td>['boehner', 'just', 'want', 'wife', 'listen', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>jk rowling wishes snape happy birthday in the ...</td>\n",
       "      <td>['jk', 'rowling', 'wishes', 'snape', 'happy', ...</td>\n",
       "      <td>['jk', 'rowling', 'wish', 'snape', 'happy', 'b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  former versace store clerk sues over secret 'b...   \n",
       "1  the 'roseanne' revival catches up to our thorn...   \n",
       "2  mom starting to fear son's web series closest ...   \n",
       "3  boehner just wants wife to listen, not come up...   \n",
       "4  j.k. rowling wishes snape happy birthday in th...   \n",
       "\n",
       "                                    headline_cleaned  \\\n",
       "0  former versace store clerk sues over secret bl...   \n",
       "1  the roseanne revival catches up to our thorny ...   \n",
       "2  mom starting to fear sons web series closest t...   \n",
       "3  boehner just wants wife to listen not come up ...   \n",
       "4  jk rowling wishes snape happy birthday in the ...   \n",
       "\n",
       "                                 tokenized_text_nltk  \\\n",
       "0  ['former', 'versace', 'store', 'clerk', 'sues'...   \n",
       "1  ['the', 'roseanne', 'revival', 'catches', 'up'...   \n",
       "2  ['mom', 'starting', 'to', 'fear', 'sons', 'web...   \n",
       "3  ['boehner', 'just', 'wants', 'wife', 'to', 'li...   \n",
       "4  ['jk', 'rowling', 'wishes', 'snape', 'happy', ...   \n",
       "\n",
       "                                lemmatized_text_nltk  \n",
       "0  ['former', 'versace', 'store', 'clerk', 'sue',...  \n",
       "1  ['roseanne', 'revival', 'catch', 'thorny', 'po...  \n",
       "2  ['mom', 'starting', 'fear', 'son', 'web', 'ser...  \n",
       "3  ['boehner', 'just', 'want', 'wife', 'listen', ...  \n",
       "4  ['jk', 'rowling', 'wish', 'snape', 'happy', 'b...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_df = pd.read_csv('data/features_nltk.csv')\n",
    "spacy_df = spacy_df[['headline', 'headline_cleaned', 'tokenized_text_nltk', 'lemmatized_text_nltk']]\n",
    "spacy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_df['tokenized_text_nltk'] = spacy_df['tokenized_text_nltk'].apply(lambda x: x.replace('[','').replace(']','').replace(\"'\",'').replace(\",\",''))\n",
    "spacy_df['tokenized_text_nltk'] = spacy_df['tokenized_text_nltk'].apply(lambda x: x.split())\n",
    "spacy_df['lemmatized_text_nltk'] = spacy_df['lemmatized_text_nltk'].apply(lambda x: x.replace('[','').replace(']','').replace(\"'\",'').replace(\",\",''))\n",
    "spacy_df['lemmatized_text_nltk'] = spacy_df['lemmatized_text_nltk'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['former', 'versace', 'store', 'clerk', 'sues', 'over', 'secret', 'black', 'code', 'for', 'minority', 'shoppers']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = spacy_df['tokenized_text_nltk'].tolist()\n",
    "lemmatized_sentences = spacy_df['lemmatized_text_nltk'].tolist()\n",
    "\n",
    "print(tokenized_sentences[0])\n",
    "\n",
    "\n",
    "# Train Word2Vec models\n",
    "w2v_tokenized = Word2Vec(tokenized_sentences, min_count=1, vector_size=100)\n",
    "w2v_lemmatized = Word2Vec(lemmatized_sentences, min_count=1, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenized_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get word embeddings for a sentence\n",
    "def get_word2vec_embeddings(model, sentence):\n",
    "    embeddings = []\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            embeddings.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            embeddings.append([0]*100)  # Default vector if word not found\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word embeddings for tokenized and lemmatized sentences (for each word in the sentence)\n",
    "spacy_df['tokenized_word2vec'] = spacy_df['tokenized_text_nltk'].apply(lambda x: [get_word2vec_embeddings(w2v_tokenized, y) for y in x])\n",
    "spacy_df['lemmatized_word2vec'] = spacy_df['lemmatized_text_nltk'].apply(lambda x: [get_word2vec_embeddings(w2v_lemmatized, y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>headline_cleaned</th>\n",
       "      <th>tokenized_text_nltk</th>\n",
       "      <th>lemmatized_text_nltk</th>\n",
       "      <th>tokenized_word2vec</th>\n",
       "      <th>lemmatized_word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>former versace store clerk sues over secret bl...</td>\n",
       "      <td>[former, versace, store, clerk, sues, over, se...</td>\n",
       "      <td>[former, versace, store, clerk, sue, secret, b...</td>\n",
       "      <td>[[[-0.44300553, 0.5421461, 0.17735429, 0.12578...</td>\n",
       "      <td>[[[-0.3604961, 0.42469707, 0.24075222, 0.05691...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>the roseanne revival catches up to our thorny ...</td>\n",
       "      <td>[the, roseanne, revival, catches, up, to, our,...</td>\n",
       "      <td>[roseanne, revival, catch, thorny, political, ...</td>\n",
       "      <td>[[[-0.34196183, 0.79046404, 0.36153653, 0.2158...</td>\n",
       "      <td>[[[-0.025584826, 0.013280583, 0.00817118, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>mom starting to fear sons web series closest t...</td>\n",
       "      <td>[mom, starting, to, fear, sons, web, series, c...</td>\n",
       "      <td>[mom, starting, fear, son, web, series, closes...</td>\n",
       "      <td>[[[-0.43636551, 0.51213676, 0.15836011, 0.1524...</td>\n",
       "      <td>[[[-0.5373482, 0.63286495, 0.34453112, 0.14433...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>boehner just wants wife to listen not come up ...</td>\n",
       "      <td>[boehner, just, wants, wife, to, listen, not, ...</td>\n",
       "      <td>[boehner, just, want, wife, listen, not, come,...</td>\n",
       "      <td>[[[-0.103065215, 0.11661247, 0.057135496, 0.02...</td>\n",
       "      <td>[[[-0.09294353, 0.09803749, 0.053146668, 0.020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>jk rowling wishes snape happy birthday in the ...</td>\n",
       "      <td>[jk, rowling, wishes, snape, happy, birthday, ...</td>\n",
       "      <td>[jk, rowling, wish, snape, happy, birthday, mo...</td>\n",
       "      <td>[[[-0.025916466, 0.023986552, 0.016223146, 0.0...</td>\n",
       "      <td>[[[-0.027325345, 0.032817025, 0.018137066, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  former versace store clerk sues over secret 'b...   \n",
       "1  the 'roseanne' revival catches up to our thorn...   \n",
       "2  mom starting to fear son's web series closest ...   \n",
       "3  boehner just wants wife to listen, not come up...   \n",
       "4  j.k. rowling wishes snape happy birthday in th...   \n",
       "\n",
       "                                    headline_cleaned  \\\n",
       "0  former versace store clerk sues over secret bl...   \n",
       "1  the roseanne revival catches up to our thorny ...   \n",
       "2  mom starting to fear sons web series closest t...   \n",
       "3  boehner just wants wife to listen not come up ...   \n",
       "4  jk rowling wishes snape happy birthday in the ...   \n",
       "\n",
       "                                 tokenized_text_nltk  \\\n",
       "0  [former, versace, store, clerk, sues, over, se...   \n",
       "1  [the, roseanne, revival, catches, up, to, our,...   \n",
       "2  [mom, starting, to, fear, sons, web, series, c...   \n",
       "3  [boehner, just, wants, wife, to, listen, not, ...   \n",
       "4  [jk, rowling, wishes, snape, happy, birthday, ...   \n",
       "\n",
       "                                lemmatized_text_nltk  \\\n",
       "0  [former, versace, store, clerk, sue, secret, b...   \n",
       "1  [roseanne, revival, catch, thorny, political, ...   \n",
       "2  [mom, starting, fear, son, web, series, closes...   \n",
       "3  [boehner, just, want, wife, listen, not, come,...   \n",
       "4  [jk, rowling, wish, snape, happy, birthday, mo...   \n",
       "\n",
       "                                  tokenized_word2vec  \\\n",
       "0  [[[-0.44300553, 0.5421461, 0.17735429, 0.12578...   \n",
       "1  [[[-0.34196183, 0.79046404, 0.36153653, 0.2158...   \n",
       "2  [[[-0.43636551, 0.51213676, 0.15836011, 0.1524...   \n",
       "3  [[[-0.103065215, 0.11661247, 0.057135496, 0.02...   \n",
       "4  [[[-0.025916466, 0.023986552, 0.016223146, 0.0...   \n",
       "\n",
       "                                 lemmatized_word2vec  \n",
       "0  [[[-0.3604961, 0.42469707, 0.24075222, 0.05691...  \n",
       "1  [[[-0.025584826, 0.013280583, 0.00817118, 0.00...  \n",
       "2  [[[-0.5373482, 0.63286495, 0.34453112, 0.14433...  \n",
       "3  [[[-0.09294353, 0.09803749, 0.053146668, 0.020...  \n",
       "4  [[[-0.027325345, 0.032817025, 0.018137066, 0.0...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.matutils import corpus2dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "tokenized_dict = Dictionary(tokenized_sentences)\n",
    "lemmatized_dict = Dictionary(lemmatized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to bag-of-words format\n",
    "tokenized_corpus = [tokenized_dict.doc2bow(text) for text in tokenized_sentences]\n",
    "lemmatized_corpus = [lemmatized_dict.doc2bow(text) for text in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA models\n",
    "lda_tokenized = LdaModel(tokenized_corpus, num_topics=10, id2word=tokenized_dict)\n",
    "lda_lemmatized = LdaModel(lemmatized_corpus, num_topics=10, id2word=lemmatized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distributions for each document\n",
    "tokenized_topics = lda_tokenized.get_document_topics(tokenized_corpus)\n",
    "lemmatized_topics = lda_lemmatized.get_document_topics(lemmatized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.14148138),\n",
       " (1, 0.09167796),\n",
       " (4, 0.5092524),\n",
       " (6, 0.08440458),\n",
       " (8, 0.12791975)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert topic distributions to dense vectors\n",
    "def get_dense_topics(topics):\n",
    "    dense_topics = []\n",
    "    for topic in topics:\n",
    "        topic_vector = [0]*10  # Assuming 10 topics\n",
    "        for topic_id, prob in topic:\n",
    "            topic_vector[topic_id] = prob\n",
    "        dense_topics.append(topic_vector)\n",
    "    return dense_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14152764, 0.09167853, 0, 0, 0.5092569, 0, 0.08435021, 0, 0.12792276, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dense_topics = get_dense_topics(tokenized_topics)\n",
    "lemmatized_dense_topics = get_dense_topics(lemmatized_topics)\n",
    "\n",
    "print(tokenized_dense_topics[0])\n",
    "\n",
    "# Add dense topic vectors to DataFrame\n",
    "spacy_df['Tokenized_lda_topics'] = tokenized_dense_topics\n",
    "spacy_df['Lemmatized_lda_topics'] = lemmatized_dense_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_df.to_csv('data/embeddings_nltk.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please avoid pushing this csv file to github. The files are large and will take 5-6 minutes to store as a csv locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer and TfidfVectorizer\n",
    "\n",
    "Owen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import CountVectorizer and TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Convert lists of tokens back to strings for vectorizers\n",
    "tokenized_texts = [' '.join(tokens) for tokens in tokenized_sentences]\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer features shape (tokenized): (26709, 1000)\n",
      "CountVectorizer features shape (lemmatized): (26709, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Implement CountVectorizer\n",
    "# Create and fit CountVectorizer (max_features=1000 to avoid overly sparse matrices)\n",
    "count_vec_tokenized = CountVectorizer(max_features=1000)\n",
    "count_vec_lemmatized = CountVectorizer(max_features=1000)\n",
    "\n",
    "\n",
    "# Transform text data into count vectors\n",
    "count_tokenized = count_vec_tokenized.fit_transform(tokenized_texts)\n",
    "count_lemmatized = count_vec_lemmatized.fit_transform(lemmatized_texts)\n",
    "\n",
    "# Get feature names\n",
    "tokenized_count_features = count_vec_tokenized.get_feature_names_out()\n",
    "lemmatized_count_features = count_vec_lemmatized.get_feature_names_out()\n",
    "\n",
    "print(f\"CountVectorizer features shape (tokenized): {count_tokenized.shape}\")\n",
    "print(f\"CountVectorizer features shape (lemmatized): {count_lemmatized.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer features shape (tokenized): (26709, 1000)\n",
      "TfidfVectorizer features shape (lemmatized): (26709, 1000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 4: Implement TfidfVectorizer\n",
    "# Create and fit TfidfVectorizer (max_features=1000 to avoid overly sparse matrices)\n",
    "tfidf_vec_tokenized = TfidfVectorizer(max_features=1000)\n",
    "tfidf_vec_lemmatized = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# Transform text data into TF-IDF vectors\n",
    "tfidf_tokenized = tfidf_vec_tokenized.fit_transform(tokenized_texts)\n",
    "tfidf_lemmatized = tfidf_vec_lemmatized.fit_transform(lemmatized_texts)\n",
    "\n",
    "# Get feature names\n",
    "tokenized_tfidf_features = tfidf_vec_tokenized.get_feature_names_out()\n",
    "lemmatized_tfidf_features = tfidf_vec_lemmatized.get_feature_names_out()\n",
    "\n",
    "print(f\"TfidfVectorizer features shape (tokenized): {tfidf_tokenized.shape}\")\n",
    "print(f\"TfidfVectorizer features shape (lemmatized): {tfidf_lemmatized.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 5: Create function to extract top features\n",
    "# Function to extract top features from each document\n",
    "def get_top_features(matrix, feature_names, top_n=10):\n",
    "    top_features_per_doc = []\n",
    "    \n",
    "    for i in range(matrix.shape[0]):\n",
    "        # Get feature values for document i\n",
    "        doc_features = matrix[i].toarray().flatten()\n",
    "        # Get indices of top features (by value)\n",
    "        top_indices = doc_features.argsort()[-top_n:][::-1]\n",
    "        # Get feature names and their values\n",
    "        top_features = [(feature_names[idx], doc_features[idx]) for idx in top_indices]\n",
    "        top_features_per_doc.append(top_features)\n",
    "    \n",
    "    return top_features_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 6: Get top features and add to dataframe\n",
    "# Get top 5 features for each document\n",
    "tokenized_count_top = get_top_features(count_tokenized, tokenized_count_features, 5)\n",
    "lemmatized_count_top = get_top_features(count_lemmatized, lemmatized_count_features, 5)\n",
    "tokenized_tfidf_top = get_top_features(tfidf_tokenized, tokenized_tfidf_features, 5)\n",
    "lemmatized_tfidf_top = get_top_features(tfidf_lemmatized, lemmatized_tfidf_features, 5)\n",
    "\n",
    "# Add to dataframe\n",
    "spacy_df['tokenized_count_top'] = tokenized_count_top\n",
    "spacy_df['lemmatized_count_top'] = lemmatized_count_top\n",
    "spacy_df['tokenized_tfidf_top'] = tokenized_tfidf_top\n",
    "spacy_df['lemmatized_tfidf_top'] = lemmatized_tfidf_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Add dense feature vectors to dataframe\n",
    "# Convert sparse matrices to dense arrays for selected features\n",
    "# We'll take the first 100 features to avoid making the dataframe too large\n",
    "tokenized_count_dense = count_tokenized[:, :100].toarray()\n",
    "lemmatized_count_dense = count_lemmatized[:, :100].toarray()\n",
    "tokenized_tfidf_dense = tfidf_tokenized[:, :100].toarray()\n",
    "lemmatized_tfidf_dense = tfidf_lemmatized[:, :100].toarray()\n",
    "\n",
    "# Add count vector features to dataframe\n",
    "spacy_df['tokenized_count_features'] = tokenized_count_dense.tolist()\n",
    "spacy_df['lemmatized_count_features'] = lemmatized_count_dense.tolist()\n",
    "spacy_df['tokenized_tfidf_features'] = tokenized_tfidf_dense.tolist()\n",
    "spacy_df['lemmatized_tfidf_features'] = lemmatized_tfidf_dense.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: former versace store clerk sues over secret 'black code' for minority shoppers\n",
      "Top TFIDF features (tokenized): [('store', 0.5083117545808206), ('secret', 0.4641108821678654), ('former', 0.4520097563458886), ('black', 0.3928886613364516), ('over', 0.34817736616156264)]\n",
      "\n",
      "\n",
      "Headline: the 'roseanne' revival catches up to our thorny political mood, for better and worse\n",
      "Top TFIDF features (tokenized): [('worse', 0.4915417386795098), ('political', 0.43993415731592855), ('better', 0.41459558156287946), ('our', 0.3925549641699866), ('up', 0.29638310190796713)]\n",
      "\n",
      "\n",
      "Headline: mom starting to fear son's web series closest thing she will have to grandchild\n",
      "Top TFIDF features (tokenized): [('starting', 0.4073715548705109), ('fear', 0.4061818705229094), ('series', 0.3899743530784772), ('thing', 0.344629484832939), ('mom', 0.32718931745424296)]\n",
      "\n",
      "\n",
      "Headline: boehner just wants wife to listen, not come up with alternative debt-reduction ideas\n",
      "Top TFIDF features (tokenized): [('ideas', 0.4591788451187499), ('wife', 0.4229050471427014), ('come', 0.4058643150113101), ('wants', 0.3878852195663974), ('just', 0.2994516209816602)]\n",
      "\n",
      "\n",
      "Headline: j.k. rowling wishes snape happy birthday in the most magical way\n",
      "Top TFIDF features (tokenized): [('wishes', 0.4860210541824099), ('happy', 0.46104279482885613), ('birthday', 0.46001286317635764), ('way', 0.3693501708659074), ('most', 0.3685882285585116)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Display examples of top TFIDF features\n",
    "# Display example of top TFIDF features for the first few headlines\n",
    "for i in range(5):\n",
    "    print(f\"Headline: {spacy_df['headline'].iloc[i]}\")\n",
    "    print(f\"Top TFIDF features (tokenized): {spacy_df['tokenized_tfidf_top'].iloc[i]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save the updated dataframe\n",
    "# Replace your existing save cell with this\n",
    "spacy_df.to_csv('data/embeddings_count_tfidf.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
