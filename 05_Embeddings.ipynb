{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename all _nltk with _spacy to get the results for Spacy based preprocessed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>headline_cleaned</th>\n",
       "      <th>tokenized_text_nltk</th>\n",
       "      <th>lemmatized_text_nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>former versace store clerk sues over secret bl...</td>\n",
       "      <td>['former', 'versace', 'store', 'clerk', 'sues'...</td>\n",
       "      <td>['former', 'versace', 'store', 'clerk', 'sue',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>the roseanne revival catches up to our thorny ...</td>\n",
       "      <td>['the', 'roseanne', 'revival', 'catches', 'up'...</td>\n",
       "      <td>['roseanne', 'revival', 'catch', 'thorny', 'po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>mom starting to fear sons web series closest t...</td>\n",
       "      <td>['mom', 'starting', 'to', 'fear', 'sons', 'web...</td>\n",
       "      <td>['mom', 'starting', 'fear', 'son', 'web', 'ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>boehner just wants wife to listen not come up ...</td>\n",
       "      <td>['boehner', 'just', 'wants', 'wife', 'to', 'li...</td>\n",
       "      <td>['boehner', 'just', 'want', 'wife', 'listen', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>jk rowling wishes snape happy birthday in the ...</td>\n",
       "      <td>['jk', 'rowling', 'wishes', 'snape', 'happy', ...</td>\n",
       "      <td>['jk', 'rowling', 'wish', 'snape', 'happy', 'b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  former versace store clerk sues over secret 'b...   \n",
       "1  the 'roseanne' revival catches up to our thorn...   \n",
       "2  mom starting to fear son's web series closest ...   \n",
       "3  boehner just wants wife to listen, not come up...   \n",
       "4  j.k. rowling wishes snape happy birthday in th...   \n",
       "\n",
       "                                    headline_cleaned  \\\n",
       "0  former versace store clerk sues over secret bl...   \n",
       "1  the roseanne revival catches up to our thorny ...   \n",
       "2  mom starting to fear sons web series closest t...   \n",
       "3  boehner just wants wife to listen not come up ...   \n",
       "4  jk rowling wishes snape happy birthday in the ...   \n",
       "\n",
       "                                 tokenized_text_nltk  \\\n",
       "0  ['former', 'versace', 'store', 'clerk', 'sues'...   \n",
       "1  ['the', 'roseanne', 'revival', 'catches', 'up'...   \n",
       "2  ['mom', 'starting', 'to', 'fear', 'sons', 'web...   \n",
       "3  ['boehner', 'just', 'wants', 'wife', 'to', 'li...   \n",
       "4  ['jk', 'rowling', 'wishes', 'snape', 'happy', ...   \n",
       "\n",
       "                                lemmatized_text_nltk  \n",
       "0  ['former', 'versace', 'store', 'clerk', 'sue',...  \n",
       "1  ['roseanne', 'revival', 'catch', 'thorny', 'po...  \n",
       "2  ['mom', 'starting', 'fear', 'son', 'web', 'ser...  \n",
       "3  ['boehner', 'just', 'want', 'wife', 'listen', ...  \n",
       "4  ['jk', 'rowling', 'wish', 'snape', 'happy', 'b...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_df = pd.read_csv('data/features_nltk.csv')\n",
    "spacy_df = spacy_df[['headline', 'headline_cleaned', 'tokenized_text_nltk', 'lemmatized_text_nltk']]\n",
    "spacy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_df['tokenized_text_nltk'] = spacy_df['tokenized_text_nltk'].apply(lambda x: x.replace('[','').replace(']','').replace(\"'\",'').replace(\",\",''))\n",
    "spacy_df['tokenized_text_nltk'] = spacy_df['tokenized_text_nltk'].apply(lambda x: x.split())\n",
    "spacy_df['lemmatized_text_nltk'] = spacy_df['lemmatized_text_nltk'].apply(lambda x: x.replace('[','').replace(']','').replace(\"'\",'').replace(\",\",''))\n",
    "spacy_df['lemmatized_text_nltk'] = spacy_df['lemmatized_text_nltk'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['former', 'versace', 'store', 'clerk', 'sues', 'over', 'secret', 'black', 'code', 'for', 'minority', 'shoppers']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = spacy_df['tokenized_text_nltk'].tolist()\n",
    "lemmatized_sentences = spacy_df['lemmatized_text_nltk'].tolist()\n",
    "\n",
    "print(tokenized_sentences[0])\n",
    "\n",
    "\n",
    "# Train Word2Vec models\n",
    "w2v_tokenized = Word2Vec(tokenized_sentences, min_count=1, vector_size=100)\n",
    "w2v_lemmatized = Word2Vec(lemmatized_sentences, min_count=1, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenized_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get word embeddings for a sentence\n",
    "def get_word2vec_embeddings(model, sentence):\n",
    "    embeddings = []\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            embeddings.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            embeddings.append([0]*100)  # Default vector if word not found\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word embeddings for tokenized and lemmatized sentences (for each word in the sentence)\n",
    "spacy_df['tokenized_word2vec'] = spacy_df['tokenized_text_nltk'].apply(lambda x: [get_word2vec_embeddings(w2v_tokenized, y) for y in x])\n",
    "spacy_df['lemmatized_word2vec'] = spacy_df['lemmatized_text_nltk'].apply(lambda x: [get_word2vec_embeddings(w2v_lemmatized, y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>headline_cleaned</th>\n",
       "      <th>tokenized_text_nltk</th>\n",
       "      <th>lemmatized_text_nltk</th>\n",
       "      <th>tokenized_word2vec</th>\n",
       "      <th>lemmatized_word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>former versace store clerk sues over secret bl...</td>\n",
       "      <td>[former, versace, store, clerk, sues, over, se...</td>\n",
       "      <td>[former, versace, store, clerk, sue, secret, b...</td>\n",
       "      <td>[[[-0.4229426, 0.55270875, 0.19162792, 0.13248...</td>\n",
       "      <td>[[[-0.31757945, 0.4251927, 0.18504636, 0.11165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>the roseanne revival catches up to our thorny ...</td>\n",
       "      <td>[the, roseanne, revival, catches, up, to, our,...</td>\n",
       "      <td>[roseanne, revival, catch, thorny, political, ...</td>\n",
       "      <td>[[[-0.52555096, 0.723542, 0.41971695, 0.204297...</td>\n",
       "      <td>[[[-0.02185243, 0.012011358, 0.005177774, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>mom starting to fear sons web series closest t...</td>\n",
       "      <td>[mom, starting, to, fear, sons, web, series, c...</td>\n",
       "      <td>[mom, starting, fear, son, web, series, closes...</td>\n",
       "      <td>[[[-0.44208562, 0.54864866, 0.16279277, 0.1702...</td>\n",
       "      <td>[[[-0.46234107, 0.64025134, 0.25795487, 0.2200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>boehner just wants wife to listen not come up ...</td>\n",
       "      <td>[boehner, just, wants, wife, to, listen, not, ...</td>\n",
       "      <td>[boehner, just, want, wife, listen, not, come,...</td>\n",
       "      <td>[[[-0.10652897, 0.12577043, 0.057611495, 0.029...</td>\n",
       "      <td>[[[-0.0803288, 0.09589496, 0.03889328, 0.03054...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>jk rowling wishes snape happy birthday in the ...</td>\n",
       "      <td>[jk, rowling, wishes, snape, happy, birthday, ...</td>\n",
       "      <td>[jk, rowling, wish, snape, happy, birthday, mo...</td>\n",
       "      <td>[[[-0.032667488, 0.035985168, 0.01892513, 0.01...</td>\n",
       "      <td>[[[-0.017622098, 0.023935243, 0.010494243, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  former versace store clerk sues over secret 'b...   \n",
       "1  the 'roseanne' revival catches up to our thorn...   \n",
       "2  mom starting to fear son's web series closest ...   \n",
       "3  boehner just wants wife to listen, not come up...   \n",
       "4  j.k. rowling wishes snape happy birthday in th...   \n",
       "\n",
       "                                    headline_cleaned  \\\n",
       "0  former versace store clerk sues over secret bl...   \n",
       "1  the roseanne revival catches up to our thorny ...   \n",
       "2  mom starting to fear sons web series closest t...   \n",
       "3  boehner just wants wife to listen not come up ...   \n",
       "4  jk rowling wishes snape happy birthday in the ...   \n",
       "\n",
       "                                 tokenized_text_nltk  \\\n",
       "0  [former, versace, store, clerk, sues, over, se...   \n",
       "1  [the, roseanne, revival, catches, up, to, our,...   \n",
       "2  [mom, starting, to, fear, sons, web, series, c...   \n",
       "3  [boehner, just, wants, wife, to, listen, not, ...   \n",
       "4  [jk, rowling, wishes, snape, happy, birthday, ...   \n",
       "\n",
       "                                lemmatized_text_nltk  \\\n",
       "0  [former, versace, store, clerk, sue, secret, b...   \n",
       "1  [roseanne, revival, catch, thorny, political, ...   \n",
       "2  [mom, starting, fear, son, web, series, closes...   \n",
       "3  [boehner, just, want, wife, listen, not, come,...   \n",
       "4  [jk, rowling, wish, snape, happy, birthday, mo...   \n",
       "\n",
       "                                  tokenized_word2vec  \\\n",
       "0  [[[-0.4229426, 0.55270875, 0.19162792, 0.13248...   \n",
       "1  [[[-0.52555096, 0.723542, 0.41971695, 0.204297...   \n",
       "2  [[[-0.44208562, 0.54864866, 0.16279277, 0.1702...   \n",
       "3  [[[-0.10652897, 0.12577043, 0.057611495, 0.029...   \n",
       "4  [[[-0.032667488, 0.035985168, 0.01892513, 0.01...   \n",
       "\n",
       "                                 lemmatized_word2vec  \n",
       "0  [[[-0.31757945, 0.4251927, 0.18504636, 0.11165...  \n",
       "1  [[[-0.02185243, 0.012011358, 0.005177774, 0.00...  \n",
       "2  [[[-0.46234107, 0.64025134, 0.25795487, 0.2200...  \n",
       "3  [[[-0.0803288, 0.09589496, 0.03889328, 0.03054...  \n",
       "4  [[[-0.017622098, 0.023935243, 0.010494243, 0.0...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.matutils import corpus2dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "tokenized_dict = Dictionary(tokenized_sentences)\n",
    "lemmatized_dict = Dictionary(lemmatized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to bag-of-words format\n",
    "tokenized_corpus = [tokenized_dict.doc2bow(text) for text in tokenized_sentences]\n",
    "lemmatized_corpus = [lemmatized_dict.doc2bow(text) for text in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA models\n",
    "lda_tokenized = LdaModel(tokenized_corpus, num_topics=10, id2word=tokenized_dict)\n",
    "lda_lemmatized = LdaModel(lemmatized_corpus, num_topics=10, id2word=lemmatized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distributions for each document\n",
    "tokenized_topics = lda_tokenized.get_document_topics(tokenized_corpus)\n",
    "lemmatized_topics = lda_lemmatized.get_document_topics(lemmatized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.6944189), (9, 0.23896416)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert topic distributions to dense vectors\n",
    "def get_dense_topics(topics):\n",
    "    dense_topics = []\n",
    "    for topic in topics:\n",
    "        topic_vector = [0]*10  # Assuming 10 topics\n",
    "        for topic_id, prob in topic:\n",
    "            topic_vector[topic_id] = prob\n",
    "        dense_topics.append(topic_vector)\n",
    "    return dense_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0.6943857, 0, 0, 0, 0, 0, 0.23899733]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dense_topics = get_dense_topics(tokenized_topics)\n",
    "lemmatized_dense_topics = get_dense_topics(lemmatized_topics)\n",
    "\n",
    "print(tokenized_dense_topics[0])\n",
    "\n",
    "# Add dense topic vectors to DataFrame\n",
    "spacy_df['Tokenized_lda_topics'] = tokenized_dense_topics\n",
    "spacy_df['Lemmatized_lda_topics'] = lemmatized_dense_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_df.to_csv('data/embeddings_nltk.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please avoid pushing this csv file to github. The files are large and will take 5-6 minutes to store as a csv locally"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
